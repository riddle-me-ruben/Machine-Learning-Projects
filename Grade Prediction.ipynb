{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBgVA5y17mr/rUpbbI8W0k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":52,"metadata":{"id":"H_BEoberaUfI","executionInfo":{"status":"ok","timestamp":1725571209512,"user_tz":360,"elapsed":194,"user":{"displayName":"Ruben","userId":"07195949546044864784"}}},"outputs":[],"source":["# import the libraries\n","from sklearn.datasets import make_blobs # for creating random data\n","from sklearn.model_selection import train_test_split # for splitting the data\n","import matplotlib.pyplot as plt # for plotting the data\n","import numpy as np # to perform operations on arrays\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from sklearn.metrics import accuracy_score # to compute the accuracy (we will also calculate the accuracy ourselves)"]},{"cell_type":"code","source":["# create artificial data\n","X, y = make_blobs(\n","\n","    n_samples = [20,40,40,100], # the number of samples for each cluster (200 samples total)\n","    # a cluster is a group of close data\n","\n","\n","    random_state = 2, # random seed\n","\n","    centers = [[35, 30], [40, 60], [70, 40], [65, 80]], # the coordinates of the center of each cluster (each point has two features)\n","\n","    cluster_std = [10,10,10,15] # specifies the standard deviation for each cluster\n","\n",")\n","\n","# converts the y array, which contains the cluster labels (0,1,2,3) into a binary classification problem where only samples belonging to cluster #3 are labeled as 1\n","# , and all other samples labeled as 0.\n","y = np.int16(y == 3) # convert to integers (make only class 3 pass (highest grades), all other classes fail)"],"metadata":{"id":"bDGldfIxa2Mi","executionInfo":{"status":"ok","timestamp":1725571209677,"user_tz":360,"elapsed":4,"user":{"displayName":"Ruben","userId":"07195949546044864784"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["Here, $X$ is a 200-by-2 array, representing 200 instances, each with two features (the results for exams 1 and 2), and $y$ is a 1-D array of length 200 containing the class of the instances in the dataset. Thus, for $0\\leq i <200$,\n","$X[i,0]$ is the grade student $i$ obtained in exam 1, $X[i,1]$ is the grade for exam 2 and\n","$y[i]$ is student $i$'s final result, $y[i]=1$ means the student passed the course and  $y[i]=0$ means he/she did not."],"metadata":{"id":"Pe4Huk33kJ66"}},{"cell_type":"code","source":["# split the data\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, # feature array\n","    y, # target array\n","    test_size = 0.50, # 50% of data used for testing, other 50% used for training\n","    random_state = 0 # reproducibility\n",")"],"metadata":{"id":"lYaz1UBrl33q","executionInfo":{"status":"ok","timestamp":1725571209677,"user_tz":360,"elapsed":3,"user":{"displayName":"Ruben","userId":"07195949546044864784"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{"id":"ffNLaEPSCgwj"}},{"cell_type":"code","source":["# print(np.append(X_train, y_train.reshape(-1, 1), axis = 1)[:5])\n","# print(np.append(X_test, y_test.reshape(-1, 1), axis = 1)[:5])\n","\n","logistic_regression = LogisticRegression()\n","logistic_regression.fit(X_train, y_train) # train the data by invoking .fit() using the training data\n","\n","predictions = logistic_regression.predict(X_test) # make prediction on test data\n","\n","# m = predictions.shape[0] # number of samples in the testing data\n","# matches = 0\n","# for i in np.arange(m):\n","#   if predictions[i] == y_test[i]:\n","#     matches += 1\n","# print(matches / m) # how many matches were correct?\n","\n","\n","logistic_regression.score(X_test, y_test) # compare prediction to the actual (the function will internally call predict(X_test) to generate predicted labels for the test data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pI8eU0AY3QJK","executionInfo":{"status":"ok","timestamp":1725571854961,"user_tz":360,"elapsed":270,"user":{"displayName":"Ruben","userId":"07195949546044864784"}},"outputId":"02b0ef42-c110-4f8b-9e6d-158d769ef97b"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.89"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["## K Nearest Neighbors"],"metadata":{"id":"X1MKEQXBCrEC"}},{"cell_type":"code","source":["k_nearest_neighbors = KNeighborsClassifier(n_neighbors=1, weights=\"distance\"), # use the closest neighbor, weights means that closer neighbors have more influence\n","\n"],"metadata":{"id":"566OrPpPCt5m","executionInfo":{"status":"ok","timestamp":1725571209677,"user_tz":360,"elapsed":2,"user":{"displayName":"Ruben","userId":"07195949546044864784"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["## Decision Tree"],"metadata":{"id":"Nd-gL2aZC2Id"}},{"cell_type":"code","source":["decision_tree = DecisionTreeClassifier(max_depth = 3) # splits data based on feature values, max depth limits the tree to a depth of 3 levels"],"metadata":{"id":"t8UKXDBfC7LA","executionInfo":{"status":"ok","timestamp":1725571209677,"user_tz":360,"elapsed":2,"user":{"displayName":"Ruben","userId":"07195949546044864784"}}},"execution_count":57,"outputs":[]}]}